\documentclass{bioinfo}
\copyrightyear{2014}
\pubyear{2014}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}
\firstpage{1}

\title[Exact sequence clustering with starcode]{Starcode: an exact algorithm for
sequence clustering}
\author[Valera Zorita \textit{et~al}]{Eduard Valera Zorita\,$^{1,2}$, Pol
Cusc\'o\,$^{1,2}$ and Guillaume Filion\,$^{1,2}$\footnote{to whom correspondence
should be addressed}}
\address{$^{1}$Genome Architecture, Gene Regulation, Stem Cells and Cancer Programme,
Centre for Genomic Regulation (CRG), Dr. Aiguader 88, 08003 Barcelona, Spain.\\
$^{2}$Universitat Pompeu Fabra (UPF), Barcelona, Spain.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}
\section{Motivation:}
The increasing throughput of sequencing technologies offers new applications
and challenges for computational biology. One such application is the use of
random barcodes to trace and quantify transcripts or lineages in experimentals
setups. The high error rate of modern sequencers calls for additional
post-processing techniques capable of detecting and reverting the misreads.
However, in the absence of a reference population, the problem amounts to
performing a pairwise comparison of all the barcodes, which is unfeasible for
excessive computational complexity.

\section{Results:}
Here we address this problem and describe an exact algorithm to determine which
pairs of sequences lie within a given Levenshtein distance. The matched pairs
are merged into clusters represented by a canonical sequence. The effiency of
starcode is attributable to the poucet search, a novel  implementation of the
Needleman-Wunsch algorithm performed on the nodes of a trie. Parallelization
achieves linear scaling of performance on multi-core machines. On the task of
clustering random barcodes, starcode outperforms short read mappers and sequence
clustering algorithms in both speed and precision.

\section{Availability and implementation:}
The C source code is available at http://github.com/gui11aume/starcode.

\section{Contact:} \href{guillaume.filion@gmail.com}{guillaume.filion@gmail.com}
\end{abstract}


\section{Introduction}

Sequence clustering is the process of grouping similar biological sequences.
It has been traditionally applied to identify related protein families and to
reduce sequence redundancy in databases. Recently, the advent of high throughput
sequencing has created additional needs for efficient clustering algorithms, in
particular because of the high error rate of such technologies. For instance,
the Illumina platform \citep{pmid16056220} shows a 1-2\% error rate consisting of
 substitutions near the 3' end of the read \citep{pmid18660515, pmid21576222}. The
PacBio platform shows a 15\% error rate that mostly corresponds to insertions
and deletions \citep{pmid19023044}. As a consequence, the same sequence is often
decoded in different ways, which artificially increases the diversity of the
output.

Sequencing errors can be discovered by mapping the reads onto a reference, if it
is available. When the sequences are random or drawn from an unknown
reference, clustering is the best option to tell real from spurious reads. One
such case is the use of random barcodes to track cells or transcripts
\citep{pmid18809713, pmid23953119}. Sequencing errors will create erroneous
barcodes that have to be reverted to the original sequence. Hands on experience
with real datasets show that this step becomes limiting when
the number of unique sequences is high. In search for a solution to this problem, we
realized that heuristic approaches rely on assumptions that may not hold
as technologies evolve. We therefore set out to find an exact algorithm.

The first task of clustering is a matching phase where closely related barcodes are
paired, similarly to linked nodes on a graph. The second task is the clustering proper,
where communities in this graph are merged. We called our algorithm ``starcode'', in
reference to the star shape of the graph formed by the barcodes in the same
cluster. The first version of the algorithm had the same performance regardless
of the order in which the input sequences were processed, because it did not
exploit data structuring of any kind. Exploiting the prefix redundancy of
alphabetically sorted sequences allowed us to avoid unnecessary recomputations
and gain speed. This is the rationale behind the poucet search algorithm at the
heart of the matching step.

Here we describe the starcode algorithm and we benchmark it against existing
software that perform related tasks. We show that starcode is both faster and
more precise than the alternatives, achieving perfect clustering on ideal datasets.
We further show that starcode is a good candidate to cluster k-mers from large
datasets from bacterial or even metazoan genomes.


\begin{methods}
\section{Methods}
\subsection{Inexact string matching using tries}
The matching method of starcode is based on a variation of the Needleman-Wunsch
(NW) algorithm \citep{pmid5420325}. In the original algorithm
(Figure~\ref{fig:NW}a), the Levenshtein distance between two sequences is found
by applying a recurrence relation throughout a matrix of $mn$ terms, where $m$
and $n$ are the respective sequence lengths. The complexity of this dynamic
programming approach is $O(mn)$. 

\begin{figure}[!tpb]
\centerline{\includegraphics{NW.pdf}}
\caption{Needleman-Wunsch (NW) alignment. \textbf{a}
  Alignment of GTTGCA and GATCCA using
the NW alignment matrix. The margins (purple) are initialized and the cells of
the matrix (yellow) are computed from left to right and from top to bottom by
the following
dynamic programming algorithm. The value in each cell is computed from the values
of the neighboring top, left and top-left cells. Denoting $a$, $b$ and $c$ these
respective terms, the value in the cell of coordinates $(i,j)$ is computed as
$\min(a+1, b+1, c)$ if the $i$-th letter from the first sequence is identical to
the $j$-th letter from the second, or $\min(a+1, b+1, c+1)$ if the letters are
different. The Levenshtein distance
between the two sequences is found in the bottom right cell. \textbf{b}
Lower complexity algorithm to
determine whether GTTGCA and GATCCA are 2-neighbors. The values in purple cells
are set during initialization. The dynamic programming algorithm proceeds as above,
with the difference that it is interrupted if the value of a diagonal cell (bold
borders) is larger than 2. The values in purple cells are not always the same as
their equivalent in a (purple arrow), but the values in the yellow cells are
nevertheless identical. The values of the white cells are never computed, which
contributes to making the algorithm less complex.}\label{fig:NW}
\end{figure}

In many instances, the only information of interest is to find out whether the
sequences are $d$-neighbors (separated by a distance less than or equal to a
threshold $d$).  In that case, the complexity then reduces to
$O(d \min(m,n))$ as described below \citep{ukkonen}. Instead
of initializing the margins of the matrix and computing all the
terms, the matrix is initialized as shown on Figure~\ref{fig:NW}b and only the
terms around
the diagonal are computed. If the computation of a diagonal term yields a value
greater than $d$, the distance is known to be greater than $d$ and the process
is halted. Otherwise, the bottom-right term indicates the actual distance
between the sequences, as in the NW algorithm.

This method can be used for inexact matching of sequences against a reference
set indexed as prefix tree, also known as a trie \citep{ukkonen}. The terms of the
matrix are updated row-wise as a depth-first search traverses the trie from the
root, as illustrated in Figure~\ref{fig:trie}. Every time a node is visited, the row
corresponding to its depth is recomputed. If the threshold value $d$ is exceeded
for a diagonal term, the Levenshtein distance for all the downstream sequences is
also necessarily greater than $d$. Therefore, no more hits are to be discovered
in this path and the depth-first search backtracks to the parent node. When the
process halts, every tail node
(corresponding to a sequence of the database) on the path of this search is a
$d$-neighbor of the query. This method is efficient because it eliminates large
areas of the search space, and because the NW alignment of the query with each
prefix of the database is computed only once.

\begin{figure}[!tpb]
\centerline{\includegraphics{trie.pdf}}
\caption{Trie query with NW alignment. Each sequence of the index is a path in
the trie. The query GTTGCA is written at the top of a NW matrix, which is
initialized as shown on Figure~\ref{fig:NW}b. The trie is traversed by a
depth-first search (red path) from the root. At each depth, the node added to
the path is written on the left of the NW matrix and the row is computed.
Checkpoints from 1 to 4 (circled red numbers) show the state of the NW matrix
as the search proceeds. The node labelled 3 is a leaf and thus corresponds to a
2-neighbor of the query. The search path then backtracks to the node labelled 2
and the last rows of the NW are erased. The search path then goes to node labelled
4, in which case the newly computed diagonal cell exceeds the threshold (circled
in red). Even if this node has children, they are not visited (red crosses).}
\label{fig:trie}
\end{figure}


\subsection{The poucet search algorithm}
This strategy can be
further improved. Notice that if two consecutive queries share a prefix of length
$k$, the succession of computations up to the $k$-th row of the NW matrix will be exactly
the same in both queries. To take advantage of this property, the input sequences are
sorted alphabetically in order to maximize the prefix sharing. The main idea of the
algorithm is to store the computational intermediates in the nodes of the trie and
use them later to resume the computation in the next query. This way, the search can
start at depth $k$ whenever a query shares a prefix of such length with the previous
one.

However, storing the rows of the NW matrix in the nodes, as suggested by the
approach described in the previous section is not optimal. At depth equal to the
length of the shared prefix, say $k$, the terms on the right of the diagonal were
computed using characters that belong to the previous query. Since they also depend
on the terms computed at lower depth, the search cannot restart at depth $k$. This
issue is solved by storing in each node a combination of row and column terms that
form an angle shape, looking
like a horizontally flipped L character as shown on Figure~\ref{fig:poucet}. This way, the
computation intermediates stored in a node of depth $k$ depend only on the previous
characters of the query. Using this technique, the search can effectively restart
at a depth equal to the length of the shared prefix with the previous query, thereby
averting the need to recompute the first terms of the NW alignments.

In the fairy tale ``Le Petit Poucet'', the hero seeds white pebbles for his older
brothers to find their way home, which is reminiscent of the way queries pave
 the way for the next in this algorithm. We therefore called this search
algorithm ``poucet''.

\begin{figure}[!tpb]
\centerline{\includegraphics{poucet.pdf}}
\caption{Poucet search algorithm. The algorithm proceeds with the same principles
as shown on Figure~\ref{fig:trie} with the difference that the NW matrix is not
updated row-wise, but along horizontally flipped L shapes. As the depth-first
search proceeds, these values are stored in the nodes of the trie. Only the nodes
at the top contain initialized values; for the other nodes, the values at the
border are implicitly known to be 3. Since the values in the vertical part of
the flipped L are the same for every child of the same node, they are computed
only once (purple arrow). The values in the grey cells will be computed as the
search path (red) visits the node. Storing the intermediates in the nodes allows
the next query to restart at depth $k$ if it shares a common prefix of length
$k$ with the current query.}
\label{fig:poucet}
\end{figure}

\subsection{Lookup search}
If the Levenshtein distance of two sequences of length $L$ is
less than or equal to $\tau$, there must exist a perfect match between
them of at least $k =\lfloor L / (\tau+1)\rfloor$ nucleotides. We take
advantage of this property to build a lookup table containing all such
$k$-mers of the sequences already present in the trie. When a new
sequence is queried, all its subsequences are extracted and queried
against the lookup table. If none of the subsequences are found, the
minimum distance between the query and all the sequences in the trie
is necessarily greater than $\tau$, and the trie search can therefore
be omitted. This lookup is particulary useful for long query sequences
(see \ref{performance}), where $k$ is large and the probability
of an exact match between subsequences is small unless they are
actually related, i.e. they belong to the same cluster. With this
method, the absence of match is detected at the computational cost of
several table lookups, compared to the higher cost of measuring the
Levenshtein distance between long sequences.

In general, however, $L / (\tau+1)$ is not an integer. To deal
with this complication, we divide the query into $\tau+2$ subsequences.
The first $k+1$ are called \emph{words} and have a target length
$(L-\tau) / (\tau+1)$. The last subsequence is called \emph{tail} and has
length $\tau$. The length of the words is computed by integer
division, and the remainder $R$ is distributed equally among the last
$R$ words. As a result, the words have a length that differs by at
most 1, with an aggregated length equal to $L-\tau$.

Since two exact words at different positions in the sequence shall not
represent a lookup match, we store and query the words against $k+1$
distinct lookup tables. In order to allow insertions and deletions, 
the lookup search must also query shifted versions of each word. In
the worst matching case, only one word is found and the mismatches are
evenly distributed in the query sequence. In such case, the maximum 
number of cumulative insertions/deletions at the $i$-th word is
$i-1$. Hence, the $i$-th word has to be shifted and queried against
the lookup table $2(i-1)+1$ times to cover all the possible
cases. Hence, the last word will be queried $2\tau+1$ times, including
$\tau$ shifts on the right and $\tau$ shifts on the left. For the
latter shift we make use of the $\tau$ nucleotides stored in the
tail. The total number of lookups per sequence for a given $\tau$ is
$(\tau+1)^2$.

\subsection{Seek and construct}
To reduce the size of the search space, we use a dynamic ``seek and construct''
approach whereby queries are processed meanwhile the trie is built. In other
words, each sequence is matched against the trie before it is inserted. To
illustrate why the trie does not need to contain all the sequences upon query,
assume that two sequences A and B are $d$-neighbors. A is processed first. Since
B is not yet inserted, A yields no hit. It is then inserted in the trie. At the
time B is processed, A is a hit for the query and the match A-B is discovered.
This approach guarantees that every hit is discovered, while maintaining the
trie as ``thin'' as possible, thereby reducing the search time.
The whole matching process is summarized in the pseudocode shown in
Algorithms~\ref{alg:starcode} and \ref{alg:poucet}.

\begin{algorithm}
  \caption{Starcode algorithm}
  \label{alg:starcode}
  \begin{algorithmic}[1]
    \State \textbf{Define:} $\tau$
    \State \textbf{Variables:} $seed$, $start = 0$,
    $height$, $seq$, $trie$, $lastseq$, $k$
    \State \textbf{Containers:} $hits$, $pebbles$
    \State \textsc{read} sequence file
    \State $height \gets$ \textsc{determine} maximum sequence length
    \State \textsc{pad} sequences up to $height$
    \State \textsc{sort} sequences alphabetically
    \State $k \gets$ \textsc{compute} lookup word lengths
    \State $trie \gets$ \textsc{create} an empty trie of height $height$
    \State \textsc{insert} root node of $trie$ in $pebbles$ at depth 0
    \ForAll{sequences}
    \State $seq \gets$ \textsc{get} next sequence
    \If{at least one $k$-mer of $seq$ is in the lookup table}
    \State $seed \gets$ \textsc{length} of shared prefix
    between current and next sequence
    \State $start \gets$ \textsc{length} of shared prefix
    between $seq$ and $lastseq$
    \State \textsc{clear} hits
    \State \textsc{clear} $pebbles$ at depth $>start$
    \ForAll{$pebbles$ at depth $start$}
    \State $node \gets$ \textsc{get} next node from $pebbles$
    \State \textbf{call} \textsc{poucet}($seq$, $node$, $seed$,
    $hits$, $pebbles$)
    \EndFor
    \State \textsc{process} $hits$ and \textsc{link} matches to $seq$
    \State $lastseq \gets seq$
    \EndIf
    \State \textsc{insert} $seq$ path in $trie$
    \State \textsc{insert} $seq$ $k$-mers into the lookup table
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Poucet search algorithm}
  \label{alg:poucet}
  \begin{algorithmic}[1]
    \Procedure{poucet}{$query$, $node$, $seed$, $hits$, $pebbles$}:
    \State \textsc{compute} $node$-specific column following NW
    \Comment{Fig.1}
    \ForAll{$child$ nodes in $node$}
    \State \textsc{compute} $child$-specific row following NW
    \Comment{Fig.1}
    \State \textsc{compute} center value using row and column \Comment{Fig.1}
    \If{center value $>\tau$} \Comment{Mismatches exceeded.}
    \State \textbf{continue} with next $child$
    \EndIf
    \If{$node$ depth = $height$} \Comment{Hit found.}
    \State \textsc{save} $node$ sequence in $hits$
    \State \textbf{continue} with next $child$
    \EndIf
    \If{$node$ depth $\leq seed$}
    \State \textsc{save} $node$ in $pebbles$ at current depth
    \EndIf
    \State \textbf{call} poucet($query$, $child$, $seed$, $hits$, $pebbles$)
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\subsection{Parallelization}
To parallelize the search, queries are separated into contiguous blocks after
sorting. The matching algorithm proceeds in two phases. During the
first phase, an independent trie is created and filled with the
sequences of its associated block using the seek and construct process
described above. In the second phase, all the other blocks of
sequences are queried against but not inserted into each trie built in
the first phase. If the queries are segregated into $N$ blocks, the
first phase consists of $N$ seek and construct jobs, whereas the second
consists of $N(N-1)/2$ query jobs. Since the jobs show little
dependence on each other, the matching algorithm can be efficiently
parallelized provided $N$ is larger than the number of independent
threads.
\subsection{Clustering}
Starcode implements a multi-purpose clustering algorithm called ``sphere
clustering'' \citep{pmid23953119}, and a message passing algorithm
\citep{mackay} tailored for the task of clustering random barcodes. In
sphere clustering, barcodes are sorted by frequency of occurrence and
each barcode, starting from the most frequent, can claim its
$d$-neighbors that were not already claimed. 

In message passing clustering, read counts are distributed equally
among the closest neighbors of each barcode only if they are at least
5 times more frequent. The remaining sequences at the end of the
process are considered canonical, and their associated count is the
estimated cluster size. A barcode is assigned to a cluster if all its
read counts are eventually given to the corresponding canonical
barcode. The barcodes for which the read counts are split between
different canonical barcodes are not assigned to any cluster. The
reason for imposing a factor 5 or larger in order to transfer the read
counts is that barcodes with similar frequencies are not likely
derived from each other through sequencing errors. More likely they
are either unrelated, or they both derive from another more abundant
barcode. 

\end{methods}

\section{Results}

\subsection{Performance}
\label{performance}
We measured basic performance and scalability metrics of starcode on a
dataset of pseudo random sequences (Figure~\ref{fig:perf}). The standard
configuration consists of a set of 1,000,000 sequences of length 40 running
on 1 thread and with a maximum Levenshtein distance of 3. To test the
scalability as a function of a single parameter, only the parameter under
study was modified whereas the others were kept constant. 

Figure~\ref{fig:perf}a shows the running time of starcode as a
function of the number of input sequences $n$. In double logarithmic scale
the trend is a straight line with slope 1.7, which suggests that the
running time complexity of starcode is approximately $O(n^{1.7})$.
Figure~\ref{fig:perf}b shows that the running time gros exponentially
as a function of the maximum Levenshtein distance used for clustering.
This is a common feature of exact algorithms based on trie search. As
this parameter increases, the poucet search bails out at a deeper
average depth in a trie that fans out exponentially.
The running time first increases as a function of the sequence length,
but then plummets and stays low (Figure~\ref{fig:perf}c).
Beyond a certain length, the lookup search algorithm starts to be
efficient, and most of the queries are resolved without the need for
trie-search.
Finally, in we show the scalability of starcode with increasing
number of treahds in Figure~\ref{fig:perf}d. The search algorithm is fully
parallel and the relative performance increases linearly up to 8 threads.
The bending observed thereafter has two sources. The first is 
that the input reading and clustering steps are brief but not parallel,
the second is that there is insufficient memory bandwidth to satisfy
the increased demand of memory accesses and  due to hardware limitations.


\begin{figure}[!tpb]%figurex
\centerline{\includegraphics[scale=0.47]{scalability.pdf}}
\caption{Scalability. a. Logarithm of the running time versus the
logarithm of the number of sequences to be clustered. b. Running time as
a function of the clustering distance. c. Running time versus length of
the input sequences. d. Relative performance increase for different number
of parallel threads. 
}\label{fig:perf}
\end{figure}


\subsection{Benchmark against sequence clustering algorithms}
Sequence clustering is routinely used to curate databases of non
redundant nucleotide or protein sequences. We benchmarked starcode
against the two popular sequence clustering algorithms CD-HIT
\citep{pmid23060610} and USEARCH \citep{pmid20709691}.

We generated 1 million random 40-mers duplicated 47 times. For each
40-mer, we also inserted 3 addional mutant sequences to the pool
by sampling again 3 nucleotides at random positions. The probability
that such a mutant sequence is within a distance 3 of any other
40-mer is of the order of $10^{-12}$ and can be neglected. Each
cluster thus consists of 50 sequences, of which 3 have a Levenshtein
distance 3 from the centroid of the cluster (the canonical
representant).

All software were set to run on a 6-core single-processor Intel Xeon
E5-2620 system with 50~GB of DDR3-RAM at 1333~Mhz. The execution
options for each software are summarized in
Table~\ref{tab:soft-options}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%             WHERE TO PUT THIS NOW??                               %
%Denoted as \emph{sequential R} is the
%sequential algorithm of \cite{pmid23953119} which is implemented in R
%and relies on the \texttt{stringdist} function of the
%\texttt{stringdist} package.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h]
\centering
\caption{Software execution parameters}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|}
    \hline
    starcode & \texttt{-t 20}\\ \hline
    CD-HIT 4.6.1 & \texttt{-c 0.925 -T 20 -M 0}\\ \hline
    USEARCH 7.0.1090  & \texttt{-threads 20 -cluster\_fast -id 0.925}\\ \hline
        Bowtie2 indexer & \texttt{bowtie2-build --quiet}\\ \hline
    Bowtie2 2.1.0 & \texttt{-f -a -p 20 --no-hd --very-sensitive}\\ \hline
        BWA indexer & \texttt{bwa index -a is}\\ \hline
    BWA 0.7.9a & \texttt{mem -t 20 -a -k1 -B0} \\ \hline
    GEM indexer & \texttt{gem-indexer -T 12} \\ \hline
    GEM 1.423 & \texttt{-e3 -s4 -T12 --granularity 100000}\\ \hline
    \emph{sequential R} & \texttt{method='lv', maxDist=2}\\ \hline
\end{tabular}
}
\label{tab:soft-options}
\end{table}

Using 12 cores, starcode clustered the shuffled 50 million sequences
in 5 minutes and 3 seconds without a single error (the output
consisted of 1 million clusters of size 50). There is no obvious
way to compare exact algorithms with heuristic algorithms such as
CD-HIT and USEARCH. We decided to allocate the heuristics 6 minutes
on the same dataset ($\approx$ 20\% additional time) and use the
parameters giving the best accurary under this constraint.
However, we could not find a combination of
parameters such that either of them could terminate in less than
6 minutes. To simplify the task, we extracted the unique sequences
from the input file, bringing it down to 4,000,000 and followed
the same rationale. We could not find any set of parameters such
that USEARCH could process this input in less than 6 minutes,
but CD-HIT could perform the task when run as shown in
Table~\ref{tab:soft-options}.



we decided to 
allocating them 6 minutes with the same computer resources 
USEARCH could not be
run on the same input because
The running times
of CD-HIT and USEARCH were [59] and [64] minutes respectively, and they
identified 2,362,352 and 2,739,954 clusters respectively. The results
are summarized in Figure~\ref{fig:benchmark}. Both algorithms rely on
heuristics, explaining why the number of clusters is off target. These
results clearly demonstrate that starcode outperforms other clustering
algorithms both in speed and accuracy with random input sequences.

We also compared starcode on the biological benchmark dataset proposed
by \cite{pmid23060610}.

\begin{figure}[!tpb]
\centerline{\includegraphics{benchmark.pdf}}
\caption{Benchmark of starcode against sequence clustering algorithms
and short read mappers. \textbf{a} Running time compared to CD-HIT and
USEARCH on the dataset described in the text. \textbf{b} Error rate
compared to CD-HIT and USEARCH, expressed as the percentage of clusters
failing to be merged. \textbf{c} Running time compared to Bowtie2, BWA
and GEM on this dataset. The running time is decomposed in extraction
of unique sequences (purple), Burrows-Wheeler indexing (yellow), and
query proper (white). The extraction was performed with the Linux sort
command, so the time is the same in every case. \textbf{d} Error rate
compared to Bowtie2, BWA and GEM, expressed as the proportion of matches
with a single hit. This is an underestimate of the true error rate.}
\label{fig:benchmark}
\end{figure}


\subsection{Benchmark against short read mappers}
A potential strategy to cluster short sequences is to use last generation short
read mappers to match a set of sequences against itself. Once matches are
available, several community detection algorithms can be used to identify the
clusters. We benchmarked starcode against the short read mappers Bowtie2
\citep{pmid22388286}, BWA \citep{pmid19451168} and GEM \citep{pmid23103880}.
Because short read
mappers do not perform clustering, we only evaluated their performance on the
matching problem.

For this test, we used the same dataset described above, in which every
sequence has at least 2 distinct matches at a distance 3 (mutant sequences
from the same cluster may be at a distance up to 6 from each other). This
means that we
can use matches with a single hit (themselves) as a lower bound on the inaccuracy.
We used unique sequences to build an index and queried the index with the same
sequences. Figure~\ref{fig:benchmark}c shows the running time of starcode along with the
decomposition of the running time for the three mappers. Note that the time to
extract unique sequences is always the same because we used the Linux
\texttt{sort -u} command in every case. The running time of
starcode is slightly lower than short read mappers when all steps ares
taken into account.  Figure~\ref{fig:benchmark}d shows the lower bound on
the error rate, as estimated by the proportion of single hits. BWA was
found to be the most accurate aligner with a lower bound of 31\% sequences.
with a single match. This measure is an underestimate of the true error
rate, but given that starcode is an exact algorithm, a more accurate
measure is not necessary. In conclusion, starcode can compete with the
short read aligners on the matching problem, and it provides a precision
that none of the other tools can offer.


\subsection{Clustering TRIP barcodes}
In the course of setting up the TRIP technology in our laboratory
\citep{pmid23953119}, we
realized the need to develop efficient algorithms to cluster similar sequences.
Briefly, the principle of TRIP (Thousands of Reporters Integrated in Parallel)
is to tag reporter transcripts with random barcodes and measure the abundance
of barcodes in the RNA as a proxy for gene expression. There is no reference to
match aberrant barcodes against, because the tagging sequences are unknown.
Instead, barcodes are matched against each other and clustered by similarity to
infer canonical sequences.

We tested the efficiency of starcode on the TRIP dataset from
\cite{pmid23953119}. In the experiment labelled mPGKA, we identified
24.1 million (91\%) barcode-containing reads out of 26.6 million, consisting
of approximately 223,000 and 220,000 unique barcode sequences for PCR replicates
1 and 2, respectively. Following the authors, we kept only the barcodes with
at least 5 counts and performed clustering as described: ``First we sorted
barcodes according to their counts. Then, for each barcode (starting from the
most frequent one), we identified and removed all its mutant versions, defined
as barcodes within a Hamming distance of 2.'' This method is here on referred
to as the ``sequential algorithm''. When replacing the Hamming distance by the
Levenshtein distance, starcode produced exactly the same output as the sequential
algorithm. The running time of starcode averaged over the replicates was
2.90 seconds, \textit{versus} more than 3 minutes 40 seconds, which represents a
75-fold speedup.

The performance of the sequential algorithm relies on the arbitrary exclusion
of reads with less than 5 counts, the main purpose of which is to reduce the
computational burden. When all barcodes were kept, the average running time of
starcode was 10.27 seconds, \textit{versus} $>$ 6 hours for the sequential
algorithm. Using the Hamming distance, as the authors originally suggested,
decreased the average running time of the sequential algorithm to about 3 hours
and 30 minutes (and
the output differed from that of starcode). Note that in all the cases
mentioned above, starcode was run with a single core to compare the algorithms
based on similar computer resources. In conclusion, the barcode clustering
problem can be simplified by various tricks, but starcode brings down the
running time to nearly instantaneous, and thereby obviates the need for such
arbitrary heuristics.

\subsection{Identifying enriched sequence motifs}
Sequence motifs are thought to play an important role in DNA metabolism. Key
regulators, such as transcription factors, nucleosomes and non coding RNAs have
sequence preferences targeting them to the sites where they act. Identifying
those sequences is a way to pinpoint the regulators and the mechanisms they
are involved in. However, the sequence motifs are not strictly identical
at different sites, hence they are better identified by inexact matching.
This problem becomes computationally difficult for long motifs (above 12-13
nucleotides) because of the combinatorial scaling. But as motifs become
longer, the problem of identifying abundant inexact matches becomes similar
to barcode clustering. We reasoned that starcode could also be used for the
task of identifying biologically meaningful sequence motifs.

We set up a test based on the meningitis-causing agent
\textit{Neisseria meningitidis}. The genome of this bacterium is
interspersed with a frequent 12 bp sequence known as DNA uptake
sequence \citep{pmid10673000}. We extracted the 12-mers from both
orientations of  the 2.19 Mb genome, yielding 4.39 million 12-mers,
consisting of 2.77 million unique sequences. Clustering the 12-mers
with starcode within a Levenshtein distance of 2 took less than 45
seconds with 12 cores. We identified the known DNA uptake sequence
of \textit{Neisseria meningitidis} (ATGCCGTCTGAA) as the most abundant
12-mer, with 1466 exact and 2096 inexact hits. This result testifies
to the fact that starcode can be used to identify biologically
relevant motifs in bacterial genomes.

To test starcode on another application, we used the RNA-protein
interaction data produced by RNAcompete \citep{pmid19561594}. The mammalian
splicing factor SRSF1 is known to bind RNA GA-rich motifs, but there is
some disagreement about the motif that it recognizes \citep{pmid23562324}.
For each SRSF1 replicate of the RNAcompete dataset,
we replaced the RNAcompete signals by their rank and
extracted the 10-mers from the microarray probes. The 10-mers were given
a score equal to the rank of the probe they belong, and enriched motifs
were found using the sphere clustering of starcode with maximum
Levenshtein distance 2. The score of the most enriched 10-mer is thus the
sum of the ranks of all 10-mers within this distance. Clustering the 6.3
million extracted 10-mers with 12 cores took about 20 seconds for each
replicate.  The most enriched 10-mers were AGGACACGGA, AGGACACGGA,
AGGACGGAGG, AGGACGGAGG, AGGACACGGA and AGGATACAGG. Except for the last
replicate, the motifs consist of AGGAC and GGA, with a spacer of variable
length. This suggests that the binding of SRSF1 to RNA may be flexible,
which would explain the disagreement between the motifs derived from 6-mers
or 7-mers.

\section{Discussion and conclusion}
Through the parallel poucet search algorithm, starcode implements an exact
sequence clustering algorithm that is even faster than widely used heuristics.
By design, starcode is tailored to process high throughput sequencing data on
multi-core platforms. Our benchmark shows that starcode is superior in speed
and accuracy compared to other methods on the problem of clustering short random
sequences. Part of the reason is that such task is stretching the tested
software far from their initial design. Sequence clustering algorithms are
meant to cluster long biological sequences, while short read mappers are
meant to map reads on potentially large genomes. In this respect, starcode
fills a need arising from the development of barcoding technologies.

The speed of starcode also makes it proficient at more general clustering
problems, such as identifying enriched k-mers in genomes and in experimental
data. Here we have given two examples of such applications. In the first, we
recover a known enriched 12-mer in the genome of \textit{Neisseria meningitidis}.

The current version of starcode has been mildly optimized for memory
consumption. The memory footprint depends on the number of sequences to
cluster (because the sequences of the input set are loaded in memory as a
group of tries) and on the mean number of matches per sequence. Every match
has to be stored until the clustering phase, which may represent a large
memory overhead. As counterintuitive as it may seem, long queries will usually
impose a lower memory footprint because the matches between sequences are less
frequent.

The speed of starcode could also benefit other applications, such as searching
shared subsequences during genome assembly. However, starcode supports only global
and not local alignment. A workaround is to compare k-mers as we have done
in the examples above, but this approach is expected to be memory demanding.
Even though extending the starcode algorithm to support clustering of protein
sequences is straightforward, it is not expected to meet the performance level
of the presented algorithm. On the one hand, there are more letters in the protein
alphabet, so the shared prefix between consecutive queries is inherently
shorter on average, which defeats the poucet search. On the other hand, if
a threshold distance of the order of 1 per 10 nucleotides is sufficient to
identify similar DNA sequences, a distance of 1 per 10 residues is
unrealistically low for protein sequences, and this parameter critically
determines the speed of starcode. More generally, storing computational
intermediates for shared prefixes could find some applications in other
algorithms such as short read mappers. The idea of the poucet search seems
simple in retrospect, but it is a powerful way to tap into the data
structuration provided by string sorting.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     please remove the " % " symbol from \centerline{\includegraphics{fig01.eps}}
%     as it may ignore the figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Acknowledgement}
We would like to thank...

\paragraph{Funding\textcolon}
The research leading to these results has received funding from the
Government of Catalonia (Dept. of Economy and Knowledge) and the Spanish
Ministry of Economy and Competitiveness (Centro de Excelencia Severo Ochoa
2013-2017' (SEV-2012-0208). P.C. fellowship is partly financed by the
Spanish Ministry of Economy and Competitiveness (State Training Subprogram:
predoctoral fellowships for the training of PhD students (FPI) 2013).

\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
%
%\bibliographystyle{plain}
%
\bibliography{document}


%\begin{thebibliography}{}
%\end{thebibliography}
\end{document}
